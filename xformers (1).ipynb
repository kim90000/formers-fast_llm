{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3rWkYeaxRth"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQa0ldIpxsWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "لتريع الحسابات ويعمل فى كولاب"
      ],
      "metadata": {
        "id": "y3zPd8KY4onQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/diffusers/optimization/xformers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "https://github.com/facebookresearch/xformers#installing-xformers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "https://huggingface.co/docs/diffusers/optimization/memory#memory-efficient-attention\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "https://huggingface.co/docs/diffusers/optimization/fp16"
      ],
      "metadata": {
        "id": "U4Xrij8W4m85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers"
      ],
      "metadata": {
        "id": "hPpAbaHgxsaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import xformers\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# تمكين xFormers\n",
        "model = xformers.optimize(model)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# تشغيل الاستدلال\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# استخراج النص الناتج\n",
        "output_text = tokenizer.decode(outputs.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "c_BrXwC5xsdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import xformers\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model(**inputs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "model_with_xformers = xformers.optimize(model)\n",
        "\n",
        "# تشغيل الاستدلال مع xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_with_xformers = model_with_xformers(**inputs)\n",
        "end_time = time.time()\n",
        "time_with_xformers = end_time - start_time\n",
        "\n",
        "# استخراج النص الناتج\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "output_text_with_xformers = tokenizer.decode(outputs_with_xformers.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")"
      ],
      "metadata": {
        "id": "ImMeatxWyLYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import xformers\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model(**inputs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "model_with_xformers = model\n",
        "if hasattr(model_with_xformers, 'half'):\n",
        "    model_with_xformers = model_with_xformers.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "\n",
        "# تشغيل الاستدلال مع xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_with_xformers = model_with_xformers(**inputs)\n",
        "end_time = time.time()\n",
        "time_with_xformers = end_time - start_time\n",
        "\n",
        "# استخراج النص الناتج\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "output_text_with_xformers = tokenizer.decode(outputs_with_xformers.logits.argmax(dim_-1)[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")"
      ],
      "metadata": {
        "id": "q-9oOPbqycnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T245VADWydAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model(**inputs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء إذا كانت مدعومة\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers(**inputs.to('cuda'))\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "Z_SKF1GdyjZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "import xformers.ops\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model(**inputs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "# تشغيل الاستدلال مع xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_with_xformers = model_with_xformers(**inputs.to('cuda'))\n",
        "end_time = time.time()\n",
        "time_with_xformers = end_time - start_time\n",
        "\n",
        "# استخراج النص الناتج\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "output_text_with_xformers = tokenizer.decode(outputs_with_xformers.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")"
      ],
      "metadata": {
        "id": "Pm3JVk6Vyj4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "import xformers.ops\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], max_length=50)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "# تشغيل الاستدلال مع xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), max_length=50)\n",
        "end_time = time.time()\n",
        "time_with_xformers = end_time - start_time\n",
        "\n",
        "# استخراج النص الناتج\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")"
      ],
      "metadata": {
        "id": "e6D2fi6myy44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=50, pad_token_id=pad_token_id)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), max_length=50, pad_token_id=pad_token_id)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "bwu4lyRWy-54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# أو يمكنك إضافة رمز padding جديد إلى المحول\n",
        "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=50, pad_token_id=pad_token_id)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), max_length=50, pad_token_id=pad_token_id)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "2qhJNCdgzJmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 50,\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.9,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "srpcE_L2zRKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 50,\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.9,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "E06onDzkzasQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 50,\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.9,\n",
        "    \"do_sample\": True,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "58n5DTnrznpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 50,\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.9,\n",
        "    \"do_sample\": True,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "DY6L8p91zzCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "احسن كود ف الصفحة"
      ],
      "metadata": {
        "id": "aH0R70xu0aRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # أو أي نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 50,\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.9,\n",
        "    \"do_sample\": True,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "5dwVDyam0KCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "كود جيد"
      ],
      "metadata": {
        "id": "lzUFlsH80z5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # يمكن اختيار نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 50,\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.9,\n",
        "    \"do_sample\": True,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "qJ6RvaQv0fKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "كود رائع"
      ],
      "metadata": {
        "id": "uv5llzql0_UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # يمكن اختيار نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 50,\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 0.8,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.9,\n",
        "    \"do_sample\": True,\n",
        "    \"repetition_penalty\": 1.5,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "so9gQe_Q01tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال جيد"
      ],
      "metadata": {
        "id": "Vpc7OI521Rw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # يمكن اختيار نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 50,\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 0.8,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.9,\n",
        "    \"do_sample\": True,\n",
        "    \"repetition_penalty\": 1.5,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model.half()\n",
        "    # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "    model_with_xformers = model_with_xformers.to('cuda')\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "wFBmqbgd02Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # يمكن اختيار نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 50,\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 1.0,  # زيادة تنوع النص\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.95,  # تحسين جودة النص\n",
        "    \"do_sample\": True,\n",
        "    \"repetition_penalty\": 2.0,  # تقليل التكرار\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "vs6WYrDN1VyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # يمكن اختيار نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 50,\n",
        "    \"num_beams\": 7,\n",
        "    \"temperature\": 1.2,  # زيادة تنوع النص\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.95,  # تحسين جودة النص\n",
        "    \"do_sample\": True,\n",
        "    \"repetition_penalty\": 2.5,  # تقليل التكرار\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "fvRucU0Z1WHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # يمكن اختيار نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 50,\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 1.2,  # زيادة تنوع النص\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.95,  # تحسين جودة النص\n",
        "    \"do_sample\": True,\n",
        "    \"repetition_penalty\": 2.0,  # تقليل التكرار\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "56R5Ic2x1pZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"gpt2\"  # يمكن اختيار نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"8+10=؟\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 100,  # زيادة طول النص الناتج\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 1.2,  # زيادة تنوع النص\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.95,  # تحسين جودة النص\n",
        "    \"do_sample\": True,\n",
        "    \"repetition_penalty\": 2.0,  # تقليل التكرار\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "kp1FSf342F4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qwen/Qwen2.5-Coder-0.5B-Instruct"
      ],
      "metadata": {
        "id": "qY6orT642VIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال جيد جدا"
      ],
      "metadata": {
        "id": "acn7Isac4eCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# تحميل النموذج والمحول (tokenizer)\n",
        "model_name = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"  # يمكن اختيار نموذج آخر من Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز pad_token إلى eos_token إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# إعداد الإدخال\n",
        "input_text = \"What is Python?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# إعداد attention_mask و pad_token_id\n",
        "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).int()\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# إعدادات التوليد\n",
        "generation_kwargs = {\n",
        "    \"max_length\": 100,  # زيادة طول النص الناتج\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\": 1.2,  # زيادة تنوع النص\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.95,  # تحسين جودة النص\n",
        "    \"do_sample\": True,\n",
        "    \"repetition_penalty\": 2.0,  # تقليل التكرار\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"pad_token_id\": pad_token_id\n",
        "}\n",
        "\n",
        "# تشغيل الاستدلال بدون xFormers\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs_no_xformers = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generation_kwargs)\n",
        "end_time = time.time()\n",
        "time_no_xformers = end_time - start_time\n",
        "\n",
        "# تمكين xFormers\n",
        "try:\n",
        "    import xformers.ops\n",
        "\n",
        "    model_with_xformers = model.half()  # استخدام الدقة النصفية لتحسين الأداء\n",
        "    model_with_xformers = model_with_xformers.to('cuda')  # نقل النموذج إلى GPU إذا كانت متاحة\n",
        "\n",
        "    # تشغيل الاستدلال مع xFormers\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_xformers = model_with_xformers.generate(inputs['input_ids'].to('cuda'), attention_mask=inputs['attention_mask'].to('cuda'), **generation_kwargs)\n",
        "    end_time = time.time()\n",
        "    time_with_xformers = end_time - start_time\n",
        "\n",
        "    # استخراج النص الناتج\n",
        "    output_text_with_xformers = tokenizer.decode(outputs_with_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"xFormers is not installed. Skipping the optimization step.\")\n",
        "    time_with_xformers = None\n",
        "    output_text_with_xformers = None\n",
        "\n",
        "# استخراج النص الناتج بدون xFormers\n",
        "output_text_no_xformers = tokenizer.decode(outputs_no_xformers[0], skip_special_tokens=True)\n",
        "\n",
        "# عرض النتائج\n",
        "print(f\"Output without xFormers: {output_text_no_xformers}\")\n",
        "print(f\"Time without xFormers: {time_no_xformers:.6f} seconds\")\n",
        "\n",
        "if time_with_xformers is not None:\n",
        "    print(f\"Output with xFormers: {output_text_with_xformers}\")\n",
        "    print(f\"Time with xFormers: {time_with_xformers:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Skipped xFormers optimization due to missing library.\")"
      ],
      "metadata": {
        "id": "YuJBFjIF3cgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enable_xformers_memory_efficient_attention()"
      ],
      "metadata": {
        "id": "cEOd6-l_3e-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Note that this is exact attention, not an approximation, just by calling\n",
        "xformers.ops.memory_efficient_attention"
      ],
      "metadata": {
        "id": "2xB2dxBo5Jrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mUfGZP1NBINY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    sample = pipe(\"a small cat\")\n",
        "\n",
        "# optional: You can disable it via\n",
        "# pipe.disable_xformers_memory_efficient_attention()"
      ],
      "metadata": {
        "id": "_kdmJGD45dwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "display(sample)"
      ],
      "metadata": {
        "id": "ULd2ZQmq6Qjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "image = sample.images[0]\n",
        "\n",
        "display(image)"
      ],
      "metadata": {
        "id": "NIf0CTf06THv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.enable_vae_tiling()\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "xformers.ops.memory_efficient_attention"
      ],
      "metadata": {
        "id": "vh3FRtekAyQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "TVE2mrgF6wLP"
      }
    },
    {
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "from IPython.display import display\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    sample = pipe(\"a big cat\")\n",
        "\n",
        "# Access the image from the StableDiffusionPipelineOutput object\n",
        "image = sample.images[0]\n",
        "\n",
        "# Display the image\n",
        "display(image)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zoBQy5kU6eOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "from IPython.display import display\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    sample = pipe(\"teen boy\")\n",
        "\n",
        "# Access the image from the StableDiffusionPipelineOutput object\n",
        "image = sample.images[0]\n",
        "\n",
        "# Display the image\n",
        "display(image)"
      ],
      "metadata": {
        "id": "zF12EzaW6f4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    sample = pipe(\"teen boy\")\n",
        "\n",
        "# Access the image from the StableDiffusionPipelineOutput object\n",
        "image = sample.images[0]\n",
        "\n",
        "# Display the image\n",
        "display(image)"
      ],
      "metadata": {
        "id": "2Jkkv6dW662H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    sample = pipe(\"crocodile\")\n",
        "\n",
        "# Access the image from the StableDiffusionPipelineOutput object\n",
        "image = sample.images[0]\n",
        "\n",
        "# Display the image\n",
        "display(image)"
      ],
      "metadata": {
        "id": "l-ROwNi367g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    sample = pipe(\"\")\n",
        "\n",
        "# Access the image from the StableDiffusionPipelineOutput object\n",
        "image = sample.images[0]\n",
        "\n",
        "# Display the image\n",
        "display(image)"
      ],
      "metadata": {
        "id": "SBQNsi4B7Q74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJzTKq0r7XLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wMbqlstC834Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "image = sample.images[0]\n",
        "\n",
        "display(image)"
      ],
      "metadata": {
        "id": "_82nkg59837Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "xeNQxyIy-7TH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "صح1"
      ],
      "metadata": {
        "id": "WM8h9Syu-vGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from IPython.display import display\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
        "pipe.enable_vae_slicing()\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "images = pipe([prompt] * 32).images\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-s0IjMWu83-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "صح2\n",
        "تكملة"
      ],
      "metadata": {
        "id": "1wcHfhQK-lVZ"
      }
    },
    {
      "source": [
        "# Call the pipe with your prompt and assign to sample\n",
        "sample = pipe(prompt)\n",
        "\n",
        "# Now you can access the generated image\n",
        "image = sample.images[0]\n",
        "\n",
        "display(image)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1vm3u8GC-f_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = sample.images[0]\n",
        "\n",
        "display(image)"
      ],
      "metadata": {
        "id": "6ZiUMPCH9FXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Pox Hox\n",
        "اشرح معنى pipe.enable_vae_slicing() pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "Gemini\n",
        "شرح معنى الأوامر:\n",
        "الأوامر pipe.enable_vae_slicing() و pipe.enable_xformers_memory_efficient_attention() هي طرق لتحسين أداء واستهلاك الذاكرة لعمليات الاستدلال (inference) في مكتبة Diffusers من Hugging Face، وهي تُستخدم عادةً مع نماذج توليد الصور.\n",
        "\n",
        "1. pipe.enable_vae_slicing()\n",
        "\n",
        "الوظيفة: تهدف هذه الطريقة إلى تقسيم عملية فك ترميز VAE (Variational Autoencoder) إلى أجزاء أصغر.\n",
        "الفائدة:\n",
        "تقليل استهلاك الذاكرة، خاصة عند التعامل مع صور عالية الدقة.\n",
        "تحسين أداء الاستدلال على الأجهزة ذات ذاكرة VRAM محدودة.\n",
        "التفاصيل: VAE هو جزء أساسي في نماذج توليد الصور، وهو مسؤول عن تحويل التمثيلات الكامنة (latent representations) إلى صور. تقسيم هذه العملية إلى أجزاء أصغر يُتيح معالجة الصور على مراحل دون تحميلها بالكامل في الذاكرة.\n",
        "2. pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "الوظيفة: تمكين استخدام xFormers، وهي مكتبة توفر عمليات انتباه (attention) مُحسّنة للذاكرة.\n",
        "الفائدة:\n",
        "تقليل استهلاك الذاكرة بشكل كبير أثناء عمليات الانتباه في النموذج.\n",
        "تسريع عمليات الاستدلال.\n",
        "التفاصيل: عمليات الانتباه هي جزء أساسي من العديد من نماذج التعلم العميق، لكنها يمكن أن تستهلك الكثير من الذاكرة. xFormers توفر تطبيقات بديلة لعمليات الانتباه مُحسّنة للذاكرة والأداء.\n",
        "بإختصار:\n",
        "\n",
        "pipe.enable_vae_slicing() تُستخدم لتقسيم عملية فك ترميز VAE لتقليل استهلاك الذاكرة.\n",
        "pipe.enable_xformers_memory_efficient_attention() تُستخدم لتمكين عمليات انتباه مُحسّنة للذاكرة من xFormers.\n",
        "ملاحظة:\n",
        "\n",
        "يجب تثبيت مكتبة xFormers أولاً قبل استخدام pipe.enable_xformers_memory_efficient_attention(). يمكنك تثبيتها باستخدام الأمر !pip install xformers.\n",
        "هذه الطرق مُتاحة في مكتبة Diffusers من Hugging Face.\n",
        "ستكون هذه الطرق مفيدة بشكل خاص عند استخدام نماذج توليد صور كبيرة أو التعامل مع صور عالية الدقة على"
      ],
      "metadata": {
        "id": "xU9FRkLB-Q4K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fz7-NT3r-UQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from IPython.display import display\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"black-forest-labs/FLUX.1-schnell\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
        "pipe.enable_vae_slicing()\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "images = pipe([prompt] * 32).images\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1X_u_336-9aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Call the pipe with your prompt and assign to sample\n",
        "sample = pipe(prompt)\n",
        "\n",
        "# Now you can access the generated image\n",
        "image = sample.images[0]\n",
        "\n",
        "display(image)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "K_mdfrgj-9aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAE المبلط\n",
        "تتيح معالجة VAE المتجانب أيضا العمل مع الصور الكبيرة على VRAM محدود (على سبيل المثال ، إنشاء صور بدقة 4K على 8 جيجابايت من VRAM) عن طريق تقسيم الصورة إلى مربعات متداخلة ، وفك تشفير البلاط ، ثم مزج المخرجات معا لإنشاء الصورة النهائية. يجب عليك أيضا استخدام VAE المجانب مع enable_xformers_memory_efficient_attention() لتقليل استخدام الذاكرة بشكل أكبر إذا كان لديك xFormers مثبتا.\n",
        "\n",
        "لاستخدام معالجة VAE المتجانبة، استدعاء enable_vae_tiling() على المسار قبل الاستدلال:"
      ],
      "metadata": {
        "id": "5EWyPSpaAR3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال\n",
        "\n",
        "ج1"
      ],
      "metadata": {
        "id": "iPvRTmWCF6DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "prompt = \"a beautiful landscape photograph\"\n",
        "pipe.enable_vae_tiling()\n",
        "#pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "image = pipe([prompt], width=3840, height=2224, num_inference_steps=20).images[0]"
      ],
      "metadata": {
        "id": "2RQ95CDDAGnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ج2"
      ],
      "metadata": {
        "id": "YY_9dwmEF8aP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "# ... (كودك الحالي) ...\n",
        "\n",
        "# عرض الصورة\n",
        "display(image)"
      ],
      "metadata": {
        "id": "GsCNV_P7FqVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qD1zIKJ7GOXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9cHrebGGOU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7PAy0xXiGORx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال\n",
        "ج1"
      ],
      "metadata": {
        "id": "cCPKU3HHHAFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "\n",
        "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
        "pipe.enable_sequential_cpu_offload()\n",
        "image = pipe(prompt).images[0]"
      ],
      "metadata": {
        "id": "2uD4G49uGON_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ج2"
      ],
      "metadata": {
        "id": "VAKa1GLGHCWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "# ... (كودك الحالي) ...\n",
        "\n",
        "# عرض الصورة\n",
        "display(image)"
      ],
      "metadata": {
        "id": "3CkwPv94G-Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipe.unet.conv_out.state_dict()[\"weight\"].stride())  # (2880, 9, 3, 1)\n",
        "pipe.unet.to(memory_format=torch.channels_last)  # in-place operation\n",
        "print(\n",
        "    pipe.unet.conv_out.state_dict()[\"weight\"].stride()\n",
        ")  # (2880, 1, 960, 320) having a stride of 1 for the 2nd dimension proves that it works"
      ],
      "metadata": {
        "id": "vMkf0ocRG-uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/diffusers/optimization/fp16"
      ],
      "metadata": {
        "id": "B0NQOnj0Iy6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "distilled = StableDiffusionPipeline.from_pretrained(\n",
        "    \"nota-ai/bk-sdm-small\", torch_dtype=torch.float16, use_safetensors=True,\n",
        ").to(\"cuda\")\n",
        "prompt = \"a golden vase with different flowers\"\n",
        "generator = torch.manual_seed(2023)\n",
        "image = distilled(\"a golden vase with different flowers\", num_inference_steps=25, generator=generator).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "6sfc_VqNIMHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}